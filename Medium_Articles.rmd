---
title: "List of articles posted on Medium (https://medium.com/@amorimfranchi)"
author: "Guilherme Franchi"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    fig_caption: yes
    section_number: yes
    number_sections: yes
    global_numbering: yes
    toc: yes
---


```{r setup, include=FALSE}
# Make some choices depending on who is running the script and which dataset you want to process
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      include = FALSE,
                      cache= F,
                      fig.height=6,
                      fig.width=8,
                      out.width="100%")

```

# Nov 17, 2023 - Raincloud plots for clear, precise and efficient data communication

Raw data visualization involves presenting data in its most basic form, without any manipulation or aggregation, and is fundamental for initial understanding and quality assessment of your data.

Specifically, raw data visualization allows for:

* Initial Exploration: It helps in the initial exploration of data, permitting us to get a sense of the patterns, outliers, and overall characteristics present in the raw dataset.

* Identifying Anomalies: Raw data visualization allows for the quick identification of anomalies, outliers, or errors in the dataset, which might need further investigation or cleaning.

* Understanding Data Distribution: It provides a clear understanding of the distribution of the data points, helping us comprehend the nature and spread of the dataset.

* Data Quality Assessment: By visualizing raw data, it is easier to assess the quality of the dataset, including missing values, inconsistencies, or inaccuracies.

* Contextual Understanding: Visualizing raw data aids in understanding the context in which the data was collected, helping in making informed decisions about the appropriate data preprocessing steps.

* Enhanced Communication: When collaborating with others, presenting raw data visually can facilitate better communication among team members, allowing everyone to start from the same foundational view.

Typically, (raw) data and summary statistics are presented through boxplots, including in scientific manuscripts and presentations.

Although they permit combination of many summary statistics in one chart, they may still mislead readers by not providing a clue of the sample size or underlying patterns in the data.

Even if one includes the individual data points into the boxplot, it can be clustered and not helping us see the where these points lie on.

```{r data-load-Nov17, include=T}
pacman::p_load(agridat, readxl, tidyr, tidyverse, dplyr, tibble, flextable, officer,
               doBy, FSA, DataExplorer, skimr, SmartEDA,
              ggplot2, ggpubr, ghibli, ggdist, GGally, patchwork, ggiraph,
              crosstalk, plotly, DT, patchwork)

data <- agridat::ilri.sheep

data <- data |> mutate(year=as.factor(year),
                       lamb=as.factor(lamb),
                       birthwt=as.numeric(birthwt),
                       weanwt=as.numeric(weanwt),
                       weanage=as.numeric(weanage),
                       weight_gain_gram=as.numeric(round((((weanwt-birthwt)/weanage)*1000),2),na.rm=T))

data <- subset(data, select=c(lamb,gen,weight_gain_gram))

```


(ref:bad-plot) Example of a “not-so-good” illustration of data and summary statistics. Data was retrieved from dataframe “ilri.sheep” (Baker et al., 2003; https://doi.org/10.1017/S1357729800053388) located in the R library agridat v.1.22 (Wright, 2023).

```{r bad-plot, include=T, fig.cap="(ref:bad-plot)"}
bad.plot <- ggplot(data, aes(x = gen, y = weight_gain_gram, fill=gen)) +
  scale_fill_ghibli_d("SpiritedMedium", direction = -1) +
  geom_boxplot(width = 0.5) +
  xlab('Lamb genotype') + ylab('Weight gain, in g/d') +
  ggtitle("Weight gain from birth to weaning in 4 lamb genotypes") +
  theme_classic(base_size=18, base_family="serif")+
  theme(text = element_text(size=18),
        axis.text.x = element_text(angle=0, hjust=.5, vjust = 0.5, color = "black"),
        axis.text.y = element_text(color = "black"),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position="none")+
  scale_y_continuous(breaks = seq(0, 180, by=20), limits=c(0,180), expand = c(0, 0))+
  geom_jitter(position=position_dodge(0.8))

bad.plot
```


Alternatively, a raincloud plot, which is a hybrid plot mixing a halved violin plot, a box plot, and scattered raw data, can help us visualize raw data, the distribution of the data, and key summary statistics at the same time.

The raincloud plot improves upon the traditional box plot by emphasizing multiple modes, indicating the potential existence of different groups within the data.

Unlike the box plot, which does not reveal where densities gather, the raincloud plot does precisely that!

Now it is your turn. Check out this simple raincloud plot tutorial using the R library ggdist v.3.3.0 (Kay, 2023; https://mjskay.github.io/ggdist/).


1. Load library and data
```{r data-head, include=T}
head(data,10) #shows the first 10 rows only
```

2. Plotting the raincloud using ggplot2 and ggdist

(ref:good-plot) Raincloud plot illustrating illustration of data and summary statistics. Data was retrieved from dataframe “ilri.sheep” (Baker et al., 2003; https://doi.org/10.1017/S1357729800053388) located in the R library agridat v.1.22 (Wright, 2023).

```{r good-plot, include=T, fig.cap="(ref:good-plot)"}
raincloud <- ggplot(data, aes(x = gen, y = weight_gain_gram, fill=gen)) +
  scale_fill_ghibli_d("SpiritedMedium", direction = -1) +
  geom_boxplot(width = 0.1) +
  xlab('Lamb genotype') + ylab('Weight gain, in g/d') +
  ggtitle("Weight gain from birth to weaning in 4 lamb genotypes") +
  theme_classic(base_size=18, base_family="serif")+
  theme(text = element_text(size=18),
        axis.text.x = element_text(angle=0, hjust=.5, vjust = 0.5, color = "black"),
        axis.text.y = element_text(color = "black"),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position="none")+
  scale_y_continuous(breaks = seq(0, 180, by=20), limits=c(0,180), expand = c(0, 0))+
  stat_dots(side = "left", justification = 1.12, binwidth = 1.9) +
  stat_halfeye(adjust = .5, width = .6, justification = -.2, .width = 0, point_colour = NA)

raincloud
```

```{r remove-data-Nov17, include=T}
rm(data)
```

You just learned how to make a raincloud plot with R libraries ggplot2 and ggdist! Congrats!

Stay tuned for more data visualization posts as well as more content on statistical programming and data analysis.

Cheers!

Guilherme

\newpage

# Nov 22, 2023 - Exploratory data analysis with few-line code? Yes, you can!

Exploratory Data Analysis (EDA) is a fundamental step before starting any statistical analysis. It involves summarizing the main characteristics of the data, often using visual methods like histograms, raincloud plots, box plots, and more, to understand the data structure, identify patterns, spot anomalies, and test hypotheses.

The main reasons why EDA is relevant:

* Getting to know the data: EDA helps in getting a sense of what the data looks like — its distribution, range, and features. This understanding is crucial before applying any advanced analytics or modelling;

* Identifying patterns and relationships: Through EDA, you can uncover relationships between variables, correlations, trends, and potential outliers. This insight can guide further analysis and hypothesis testing;

* Detecting anomalies or outliers: Exploring the data visually often reveals anomalies or outliers that might require special treatment or investigation;

* Feature Selection: EDA aids in selecting the most relevant features for modeling. It helps in understanding which features might be most predictive or influential;

* Data cleaning and pre-processing: EDA often highlights missing data, inconsistencies, or data quality issues that need to be addressed before further analysis;

* Hypothesis generation: EDA can inspire hypotheses that can be tested formally using statistical methods.

* Communicating insights: Visualizations generated during EDA are often used to communicate findings to stakeholders who might not be well-versed in the technical aspects of data analysis.

Below you will find 4 different R libraries/functions to perform EDA with few code lines.

## Data loading and preparation

In this article, we will use the dataset "woodman.pig" (Woodman et al., 1936; https://doi.org/10.1017/S002185960002308X) from R library agridat v.1.22 (Wright, 2023).

To make it simple, the following variables will be retained for EDA:

* pen
* treatment
* pig
* sex
* amount of feed consumed (in pounds)
* weekly weight gain (in pounds)

```{r data-load-Nov22, include=T}
install.packages(c("agridat","tidyr","tidyverse","dplyr"))

library("agridat")
library("tidyr")
library("tidyverse")
library("dplyr")

data <- agridat::woodman.pig

d <- data |> 
        mutate(pen=as.factor(pen),
               treatment=as.factor(treatment),
               pig=as.factor(pig),
               sex=as.factor(sex),
               feed_consumed=as.numeric(feed),
               weekly_weight_gain=as.numeric(g)) |>
    select(pen,treatment,pig,sex,feed_consumed,weekly_weight_gain)

head(d,5)
```

```{r rainplot, include=T, fig.cap="(ref:rainplot)"}
ggplot(d, aes(x = treatment, y = feed_consumed)) +
  geom_boxplot(width = 0.25) +
  xlab('Treatment') +
  ylab('Amount of feed consumed, in pounds') +
  #ggtitle("Weight gain from birth to weaning in 4 lamb genotypes") +
  theme_classic(base_size=18, base_family="serif")+
  theme(text = element_text(size=18),
        axis.text.x = element_text(angle=0, hjust=.5, vjust = 0.5, color = "black"),
        axis.text.y = element_text(color = "black"),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position="none")+
  scale_y_continuous(breaks = seq(400, 600, by=25), limits=c(400,600), expand = c(0, 0)) +
 stat_halfeye(adjust = .5, width = .6, justification = -.3, .width = 0, point_colour = NA) +
  geom_point(shape = 95, size = 15, alpha = .2, color = "#1D785A")
```

## Exploratory Data Analysis methods

### Option 1: skimr

The *skimr* is a helpful R library for EDA.

It provides summary statistics and visualizations to understand the structure and characteristics of your dataset.

Here are some ways to use it:

install.packages("skimr")

library(skimr)
library(dplyr) # For data manipulation (optional but often used)

summary.skimr <- skim(d)

summary.skimr.numeric <- d |> 
                         skim(where(is.numeric)) # Summarize only numerical variables

summary.skimr.numeric.facet <- d |>
                               dplyr::group_by(treatment) |>
                               skim(where(is.numeric)) #Summarize numerical variables by treatment

  
### Option 2: DataExplorer

The *DataExplorer* library is another useful tool in R for EDA.

It provides various functions to quickly explore and visualize datasets, generate summary statistics, and handle missing values.

Find some ways to use this library below:

install.packages("DataExplorer")

library(DataExplorer)
library(dplyr) # For data manipulation (optional but often used)

summary.de <- plot_intro(d) #provides a quick summary of the data types, missing values, and some summary statistics

summary.report <- create_report(d, report_title = "Exploratory Data Analysis Report") #generates a comprehensive report with summary statistics, histograms, bar plots, correlation plots, etc., for numeric and categorical variables.

summary.report.faceted <- create_report(d, report_title = "Exploratory Data Analysis Report", y = "treatment")


### Option 3: SmartEDA

The *SmartEDA* library is another powerful tool for quick data overview with increasing number of exploratory functions. Analyzing information value, weight of evidence, custom tables, summary statistics, graphical techniques will be performed for both numeric and categorical predictors.

install.packages("SmartEDA")

library(SmartEDA)

data.overview.smarteda <-  ExpData(data=d,type=1) #overview of the data
   
data.strcuture.smarteda <- ExpData(data=d,type=2) #structure of the data

report.smarteda <- ExpReport(d, label=NULL, op_file="Report.html", op_dir=getwd()) #generates data overview report in HTML format

report.smarteda.faceted <- ExpReport(d, Target="treatment", label=NULL, op_file="Report.html", op_dir=getwd()) #generates data overview report by treatment in HTML format


### Option 4: tableone

The *tableone* library is another powerful tool primarily used for creating summary statistics tables.

It is particularly useful for generating publication-ready tables that summarize characteristics of different groups within a dataset.

Here is an overview of using this library for creating summary tables:

install.packages("tableone")

library(tableone)

summary.tableone <- tableone::CreateTableOne(vars = colnames(select(d, -"treatment")), 
                                          strata = c("treatment"), data = d)
# print(summary.tableone)


EDA is typically the phase where analysts or data scientists spend a considerable amount of time because it sets the stage for the entire analysis process.

It helps in making informed decisions about which analysis techniques or models might be appropriate for the dataset and the problem at hand.

You just learned four different ways of running an exploratory data analysis with few lines of code! Well done!

As R and statistical programming evolve, new libraries and functions are being developed.

So, this list is not permanent and may not even match the specific requirements of a project. 
Therefore, you should see this article as a start point for your EDA, a way of having a quick overview of your data, and inspiration for optimization of your data preparation by combination of these methods with other EDA techniques and visualization libraries for a comprehensive analysis.

If you are aware of extra R libraries and functions to perform EDA or improve the codes used above, leave a comment below! Any improvements and extra knowledge are welcome!

Stay tuned for more data visualization posts as well as more content on statistical programming and data analysis.

Cheers!

Guilherme

```{r remove-data-Nov22, include=T}
rm(data, d)
```

\newpage

# Nov 30, 2023 - "Visualizing the number of animals slaughtered for meat in the top 4 world's largest meat producers using the R library crosstalk"

The Rmd. file "Animals slaughtered" containing the algorithm is placed in the repository "MediumBlog".

\newpage

# Dec 12, 2023 - "Spotting data outliers in R"

## What is a data outlier and its impact on statistical analysis and data interpretation?
An outlier in a dataset refers to a data point or observation that significantly deviates from other observations in the sample, which can sometimes skew statistical analyses and data interpretation.

Outliers can occur due to various reasons such as measurement and data entry errors, experimental variability, or they might even represent a genuine anomaly or rare event within the data.

An outlier often signifies problematic data, such as potential errors in coding or errors in experimental execution.

Should it be established that an outlier indeed stems from an error, it is advisable to remove or rectify the aberrant value in the analysis.

However, determining the validity of an outlier can sometimes be challenging.

Outliers might emerge from random fluctuations or reveal scientifically noteworthy insights.

Consequently, outright deletion of outlier observations is not advised.

Nonetheless, when substantial outliers are present, resorting to more robust statistical methods is suggested.

Detecting outliers is important in data analysis as they can impact the accuracy of statistical models or conclusions drawn from the data.

Data outlier detection can be performed in several ways, including graphical analysis, descriptive statistics, and formal test techniques.

In today's post I will cover three ways to spot several outliers using R. Therefore, Dixon's test and Grubb's test, which examine whether a *single* low or high value is an outlier at a time, will not be covered.


## Example dataset and visualization of potential outliers

In this article, we will use the dataset "saunders.maize.uniformity", regarding uniformity trial of maize in South Africa, stored in the R library agridat v.1.22 (Wright, 2023).

We will use the variable "yield" (yield per plot, in pounds) on the year of 1930.

```{r data-load-Dec12, include=T}
install.packages(c("agridat","tidyr","tidyverse","dplyr","ggplot2"))

library("agridat")
library("tidyr")
library("tidyverse")
library("dplyr")
library("ggplot2")

data <- agridat::saunders.maize.uniformity

data <- data |> 
        mutate(year=as.factor(year),
               yield=as.numeric(yield)) |>
        filter(year=="1930")

head(data,10)
```


```{r histogram, include=T}
hist(data$yield)
```


## Boxplot inspection and interquartile range
Boxplots are useful graphical tools to detect potential outliers. Figure \@ref(fig:boxplot) illustrates a boxplot of yield per plot on each experimental year. 

(ref:boxplot) Boxplot of yield per plot in 1930. The boxplot displays five summary statistics measures: minimum, median, first and third quartiles and maximum.

```{r boxplot, include=T, fig.cap="(ref:boxplot)"}
data |> ggplot(aes(x = "", y = yield)) +
  geom_boxplot(fill = "#0c4c8a") +
  xlab(' ') +
  ylab('Yield per plot, in pounds') +
  theme_minimal(base_size=18, base_family="serif")
```


```{r individual-outliers, include=T}
out <- boxplot.stats(data$yield)$out
out_ind <- which(data$yield %in% c(out))
data[out_ind, ]
```

It is also possible to extract the values of the potential outliers thanks to the *boxplot.stats()$out* function.

Any observation considered as outlier is found outside the interval between quartiles, aka interquartile range (IQR).

The IQR is a statistical measure used to describe the spread or dispersion of a dataset and is calculated as the difference between the third quartile (Q3) and the first quartile (Q1) in a dataset.

In this example, *16* observations (`r out`) were considered outliers according to the interquartile range criterion.

With the function *which()* we can extract the row numbers corresponding to these outliers: `r out_ind`.


## Z-score
Z-scores serve as a measure of how atypical an observation is within a dataset that adheres to a normal distribution.

They represent the number of standard deviations a particular value deviates from the mean.

For instance, a Z-score of 2 signifies that an observation lies two standard deviations above the average, whereas a Z-score of -2 indicates it is positioned two standard deviations below the mean.

A Z-score of zero denotes a value that aligns precisely with the mean.

The further away an observation’s Z-score is from zero, the more unusual it is.

A standard cut-off value for finding outliers are Z-scores of +/-3 or further from zero.

```{r z-score, include=T}
data$z_yield <- scale(data$yield)
hist(data$z_yield)

z.score.ind <- which(data$z_yield < -3 | data$z_yield > 3)
data[z.score.ind,]
```

Using the Z-score methods, 4 observations were considered outliers and their row numbers were: `r z.score.ind`.


## Rosner's generalized extreme Studentized deviate (ESD) Test
The Rosner's generalized ESD test is an extension of the Grubbs' test for detecting several outliers in a univariate data set.

The steps involved in Rosner's ESD test:

1. Identify potential outliers: The test aims to identify potential outliers by iteratively removing the suspected maximum value (or minimum value, depending on the direction of the test) from the dataset. It calculates the extreme studentized deviate for each potential outlier.

2. Calculate the Extreme Studentized Deviate: The extreme studentized deviate is essentially a measure of how extreme a data point is compared to the rest of the data. It is calculated as the difference between the suspected outlier and the sample mean, divided by the sample standard deviation. This is then divided by an estimate of the standard deviation of the suspected outlier. This gives a measure of how many standard deviations away from the mean the suspected outlier is.

3. Compare with critical values: The extreme studentized deviate values obtained are compared against critical values from the ESD distribution table based on the desired significance level and the number of data points in the dataset. If the calculated extreme studentized deviate exceeds the critical value, the suspected outlier is considered significant and removed from the dataset.

4. Iterative process: The process is repeated until either no more outliers are detected or until the number of outliers detected reaches a predetermined maximum number.

In addition to normal distribution assumption, the Rosner's ESD test assumes that the variance within the dataset is constant.

Below, you can see the R script to perform the Rosner's ESD test.

```{r rosner-test, include=TRUE}
#install.packages(c("EnvStats"))

test <- EnvStats::rosnerTest(data$yield, k = 16, alpha = 0.05) # k = 16, which is the number of suspected outliers from interquartile range criterion analysis.

test$all.stats
```

Based on the Rosner's test, only one observation was deemed outlier, which was the observation row 456 with a value of 19.9.


## Discussion of results
Z-scores and Rosner's ESD test are both used for outlier detection but operate in distinct ways.

Z-score is a measure that describes the position of a data point within a normal distribution in terms of standard deviations from the mean. It is calculated for individual data points and helps in understanding how far a particular value deviates from the mean. Z-scores are not explicitly designed for outlier detection but can be used to identify extreme values. They assess each data point's deviation from the mean based on the standard deviation of the entire dataset.

Rosner's ESD Test is explicitly designed for outlier detection. It aims to detect potential outliers by iteratively removing suspected extreme values from the dataset. It calculates extreme studentized deviates, measuring how many standard deviations an individual value deviates from the mean, but the calculation is based on the sample mean and standard deviation. The test involves sequentially identifying and removing suspected outliers until no more are found, or until a predetermined maximum number is reached.

The main difference lies in their methodologies:

- Z-scores assess individual data points' deviations from the mean using the standard deviation of the entire dataset.
- Rosner's ESD test iteratively evaluates potential outliers by comparing their deviation from the sample mean and standard deviation, removing suspected outliers sequentially.

These methods can provide different results because they operate based on different principles and approaches. Additionally, Rosner's ESD test involves an iterative process that considers each potential outlier's influence on subsequent calculations, while Z-scores analyze individual data points independently.

The outcomes might vary due to the specific nature of the methods and their sensitivity to different types of outliers or extreme values. Also, the application and interpretation of results from these methods depend on the context and assumptions of the data being analyzed.

## Conclusions
Numerous methods exist to spot outliers, but leveraging deep knowledge of variables proves vital.

Visual tools like boxplots and histograms, along with data sorting, often reveal potential outliers swiftly.

Most statistical tests for outlier detection rely on normal distribution assumptions and can be sensitive to outliers, therefore, should not be used alone.

Instead, prior experience with the data's nature and collection process as well as a trained ability to spot unusual values and use formal statistical tests should walk together.

Once again, not every outlier warrants deletion; some hold valuable insights. Outliers often offer crucial information about the subject and data collection methods. Understanding their occurrence and potential recurrence as a natural part of the process is essential.

See you in the next post!

\newpage

# Jan 05, 2024 - "An easier-to-interpret alternative to paired barplots in R"

## Introduction
The easiness to plot makes paired barcharts very popular among researchers and data scientists. However, they may become cluttered if many groups are included in the plot and often impose a challenge to scale.

In today's post, we will explore an alternative to paired barplots that can facilitate data visualization and comparison between groups.

We will look at the variation in average farm size between 1960 and 2000 in various countries. The dataset can be retrieved at Our World in Data (https://ourworldindata.org/agricultural-production).

## Loading necessary libraries and preparing our plotting theme

In the next three code chunks, we will load the necessary libraries for this exercise, load and process our data, and prepare our plotting theme.

```{r libraries, include=T}
#install.packages(c("xxx"))

library(readr)
library(tidyverse)
library(dplyr)
library(tidyr)
library(ggtext)
library(ggplot2)
library(glue)
library(ggtext)
library(showtext)
library(showtext)
library(patchwork)

```

```{r data-load, include=T}
setwd("C:/Users/au588902/Dropbox/Medium/MediumBlog")

d <- read_csv("average-farm-size.csv") |>
     mutate(country=as.factor(Entity),
               year=as.factor(Year),
            size=as.numeric(average_farm_size_ha)) |>
        filter(year=="1960" | year== "2000") |>
        filter(country=="Argentina" | country=="Brazil" | country=="Chile" | country=="India" | country=="Netherlands" | country=="Denmark" | country=="France" | country=="Germany" | country=="Spain" | country=="Italy" | country=="United States" | country=="Canada" | country=="Ireland" | country=="Uruguay" | country=="United Kingdom" | country=="Austria")

```

```{r plot-base, include=T}

my_theme <- theme_minimal(base_size = 16, base_family = 'serif') +
  theme(
    legend.position = 'none',
    plot.title.position = 'plot',
    text = element_text(color = 'grey20'),
    plot.title = element_markdown(size = 20, margin = margin(b = 5, unit = 'mm'))
  )
theme_set(my_theme)

color_palette <- c("#0072B2", "#D55E00")
names(color_palette) <- c(1960, 2000)

title_text <- glue(
  "Comparison of average fame size between <span style = 'color:{color_palette['1960']}'>1960</span> and <span style = 'color:{color_palette['2000']}'>2000</span>")

```


(ref:bar-plot) Paired barplot illustrating variation in average farm size across several countries.

```{r bar-plot, include=T, fig.cap="(ref:bar-plot)"}
badplot <- d |>
          mutate(country = fct_reorder(country, size, max)) |>
          ggplot(aes(x=size, y=country, col=year,fill=year)) +
          geom_bar(position="dodge", stat="identity") +
          labs(x = 'Average farm size, in hectares',
          y = element_blank(),
          title = title_text) +
          scale_color_manual(values = color_palette) +
          scale_fill_manual(values =color_palette)
badplot

```

Although relatively easy to plot, the paired barplot (Fig. \@ref(fig:bar-plot)) looks quite cluttered and forces the reader to move the eyes around to make comparisons. Certainly this can be improved!

The alternative is the dumbbell plot, a combination of dotplots with connecting horizontal lines. Below you can find the code chunk to draw a dumbbell plot of the same data. Keep in mind that:
- Points need to be plotted above the horizontal lines.
- Y-axis gridlines were removed, as they became superfluous.
- Dumbbells are sorted by decreasing average farm size in 2000 after using *fct_reorder()* when we computed *segment_helper*. The same step also ensured that countries where the green dot is left of the orange dot are grouped together (and vice-versa).

Not least important, the dumbbell plotting code is presented in form of function so you can decide how to change the graph without duplicating the script and save time!

```{r dumbbell-function, include=T}
create_dot_plot <- function(d, sort_var = NULL) {
  segment_helper <- d |>
    select(country, year, size) |>
    pivot_wider(names_from = year, values_from = size, names_prefix = 'year_') |>
    mutate(change = year_2000 - year_1960,
      country = fct_reorder(country, year_2000 * if_else(change < 0, -1, 1)))
  
  if (!missing(sort_var)) {
    segment_helper <- segment_helper |>
      mutate(country = fct_reorder(country, {{sort_var}} * if_else(change < 0, -1, 1)))
  }
  
  ggplot() +
    geom_segment(data = segment_helper,
      aes(y = country, yend = country, x = year_1960, xend = year_2000),
      col = 'grey30',
      size = 1.25 ) +
    geom_point(data = d,
      aes(x = size, y = country, col = year), size = 4) +
    labs(x = 'Average farm size, in ha',
      y = element_blank(),
      title = title_text,
      caption = 'Our World in Data') +
    scale_color_manual(values = color_palette) +
    theme(panel.grid.major.y = element_blank(),
      panel.grid.minor.x = element_blank()) +
    scale_x_continuous(expand = expansion(mult = 0.01))
}

```

(ref:dumbbell-plot) Dumbbell plot illustrating variation in average farm size across several countries.

```{r dumbbell-plot, include=T, fig.cap="(ref:dumbbell-plot)"}
create_dot_plot(d, desc(year_2000))

```

The dumbbell plot ((Fig. \@ref(fig:dumbbell-plot))) displays the data sorted according to the lowest average farm size in 2000. We can see that *Chile*, *Brazil*, and *India* showed a reduction in average farm size over this selected period, while other countries witnessed an increase in average farm size.

This dumbbell plot can be further improved by replacing the dumbbells with arrows and respective legends. The chunk below demonstrates how to make this change. This chunk includes the steps to make the plot and a custom legend explaining the temporal order of each arrow. The arrow plot and the custom legend were put together using the R library *patchwork*.


(ref:arrow-plot) Arrow plot illustrating variation in average farm size across several countries.

```{r arrow-plot, include=T, fig.cap="(ref:arrow-plot)"}
arrow_plot <- d |> 
  
  select(country, year, size) |> 
  
  pivot_wider(names_from = year, names_prefix = 'year_', values_from = size) |> 
  
  mutate(change = year_2000 - year_1960, 
    sign_change = (change > 0),
    country = fct_reorder(country, year_2000 * if_else(sign_change, -1, 1))) |> 
  
  ggplot( aes(x = year_1960, xend = year_2000, 
      y = country, yend = country,
      color = sign_change)) +
  geom_segment( arrow = arrow(angle = 30, length = unit(0.2, 'cm')), size = 1) +
  labs(x = 'Average farm size, in ha', 
    y = element_blank(),
    title = 'Comparison of average farm size between 1960 and 2000',
     caption = 'Our World in Data') +
  scale_color_manual( values = unname(color_palette))

dat <- tibble(country = c(1.1, 1),
  year_1960 = c(2, 1),
  year_2000 = c(1, 2)) 

dat_longer <- dat |> 
  pivot_longer( cols = -country,
    names_to = 'label',
    values_to = 'size',
    names_prefix = 'year_') 

custom_legend <- ggplot() +
  geom_rect(aes(xmin = 0.8, xmax = 2.2,
        ymin = 0.9, ymax = 1.2),
    fill = 'white',
    col = 'grey30') +
  geom_segment( data = dat,
    mapping = aes(x = year_1960, xend = year_2000, 
      y = country, yend = country),
    arrow = arrow(angle = 30, length = unit(0.2, 'cm')),
    color = color_palette,
    size = 1) +
  geom_text(data = dat_longer,
    mapping = aes(x = size, y = country, label = label),
    hjust = c(-0.1, 1.1, 1.1, -0.1),
    family = 'serif',
    color = rep(color_palette, each = 2))  +
  theme_void() +
  coord_cartesian(ylim = c(0.8, 1.3),
    xlim = c(0.75, 2.25), 
    expand = F)

arrow_plot +
  inset_element(custom_legend, left = 0.55, right = 1, top = 1, bottom = 0.8)

```

The arrow plot ((Fig. \@ref(fig:arrow-plot))) more clearly showing the directions in variation of average farm size across selected countries.

Great! We learned how to make Dumbbell and Arrow plots and, perhaps, ditch barplots. We also managed to cover the use of two extra R libraries: *glue* to annotate a nice plot title with year colors and dispensing the use of plot legends, and *patchwork* to insert annotations, shapes or illustrations into our plots.

See you next time!

\newpage


