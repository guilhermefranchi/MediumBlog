---
title: "List of articles posted on Medium (https://medium.com/@amorimfranchi)"
author: "Guilherme Franchi"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    fig_caption: yes
    section_number: yes
    number_sections: yes
    global_numbering: yes
    toc: yes
---


```{r setup, include=FALSE}
# Make some choices depending on who is running the script and which dataset you want to process
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      include = FALSE,
                      cache= F,
                      fig.height=6,
                      fig.width=8,
                      out.width="100%")

```

# Nov 17, 2023 - Raincloud plots for clear, precise and efficient data communication

Raw data visualization involves presenting data in its most basic form, without any manipulation or aggregation, and is fundamental for initial understanding and quality assessment of your data.

Specifically, raw data visualization allows for:

* Initial Exploration: It helps in the initial exploration of data, permitting us to get a sense of the patterns, outliers, and overall characteristics present in the raw dataset.

* Identifying Anomalies: Raw data visualization allows for the quick identification of anomalies, outliers, or errors in the dataset, which might need further investigation or cleaning.

* Understanding Data Distribution: It provides a clear understanding of the distribution of the data points, helping us comprehend the nature and spread of the dataset.

* Data Quality Assessment: By visualizing raw data, it is easier to assess the quality of the dataset, including missing values, inconsistencies, or inaccuracies.

* Contextual Understanding: Visualizing raw data aids in understanding the context in which the data was collected, helping in making informed decisions about the appropriate data preprocessing steps.

* Enhanced Communication: When collaborating with others, presenting raw data visually can facilitate better communication among team members, allowing everyone to start from the same foundational view.

Typically, (raw) data and summary statistics are presented through boxplots, including in scientific manuscripts and presentations.

Although they permit combination of many summary statistics in one chart, they may still mislead readers by not providing a clue of the sample size or underlying patterns in the data.

Even if one includes the individual data points into the boxplot, it can be clustered and not helping us see the where these points lie on.

```{r data-load-Nov17, include=T}
pacman::p_load(agridat, readxl, tidyr, tidyverse, dplyr, tibble, flextable, officer,
               doBy, FSA, DataExplorer, skimr, SmartEDA,
              ggplot2, ggpubr, ghibli, ggdist, GGally, patchwork, ggiraph,
              crosstalk, plotly, DT, patchwork)

data <- agridat::ilri.sheep

data <- data |> mutate(year=as.factor(year),
                       lamb=as.factor(lamb),
                       birthwt=as.numeric(birthwt),
                       weanwt=as.numeric(weanwt),
                       weanage=as.numeric(weanage),
                       weight_gain_gram=as.numeric(round((((weanwt-birthwt)/weanage)*1000),2),na.rm=T))

data <- subset(data, select=c(lamb,gen,weight_gain_gram))

```


(ref:bad-plot) Example of a “not-so-good” illustration of data and summary statistics. Data was retrieved from dataframe “ilri.sheep” (Baker et al., 2003; https://doi.org/10.1017/S1357729800053388) located in the R library agridat v.1.22 (Wright, 2023).

```{r bad-plot, include=T, fig.cap="(ref:bad-plot)"}
bad.plot <- ggplot(data, aes(x = gen, y = weight_gain_gram, fill=gen)) +
  scale_fill_ghibli_d("SpiritedMedium", direction = -1) +
  geom_boxplot(width = 0.5) +
  xlab('Lamb genotype') + ylab('Weight gain, in g/d') +
  ggtitle("Weight gain from birth to weaning in 4 lamb genotypes") +
  theme_classic(base_size=18, base_family="serif")+
  theme(text = element_text(size=18),
        axis.text.x = element_text(angle=0, hjust=.5, vjust = 0.5, color = "black"),
        axis.text.y = element_text(color = "black"),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position="none")+
  scale_y_continuous(breaks = seq(0, 180, by=20), limits=c(0,180), expand = c(0, 0))+
  geom_jitter(position=position_dodge(0.8))

bad.plot
```


Alternatively, a raincloud plot, which is a hybrid plot mixing a halved violin plot, a box plot, and scattered raw data, can help us visualize raw data, the distribution of the data, and key summary statistics at the same time.

The raincloud plot improves upon the traditional box plot by emphasizing multiple modes, indicating the potential existence of different groups within the data.

Unlike the box plot, which does not reveal where densities gather, the raincloud plot does precisely that!

Now it is your turn. Check out this simple raincloud plot tutorial using the R library ggdist v.3.3.0 (Kay, 2023; https://mjskay.github.io/ggdist/).


1. Load library and data
```{r data-head, include=T}
head(data,10) #shows the first 10 rows only
```

2. Plotting the raincloud using ggplot2 and ggdist

(ref:good-plot) Raincloud plot illustrating illustration of data and summary statistics. Data was retrieved from dataframe “ilri.sheep” (Baker et al., 2003; https://doi.org/10.1017/S1357729800053388) located in the R library agridat v.1.22 (Wright, 2023).

```{r good-plot, include=T, fig.cap="(ref:good-plot)"}
raincloud <- ggplot(data, aes(x = gen, y = weight_gain_gram, fill=gen)) +
  scale_fill_ghibli_d("SpiritedMedium", direction = -1) +
  geom_boxplot(width = 0.1) +
  xlab('Lamb genotype') + ylab('Weight gain, in g/d') +
  ggtitle("Weight gain from birth to weaning in 4 lamb genotypes") +
  theme_classic(base_size=18, base_family="serif")+
  theme(text = element_text(size=18),
        axis.text.x = element_text(angle=0, hjust=.5, vjust = 0.5, color = "black"),
        axis.text.y = element_text(color = "black"),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position="none")+
  scale_y_continuous(breaks = seq(0, 180, by=20), limits=c(0,180), expand = c(0, 0))+
  stat_dots(side = "left", justification = 1.12, binwidth = 1.9) +
  stat_halfeye(adjust = .5, width = .6, justification = -.2, .width = 0, point_colour = NA)

raincloud
```

```{r remove-data-Nov17, include=T}
rm(data)
```

You just learned how to make a raincloud plot with R libraries ggplot2 and ggdist! Congrats!

Stay tuned for more data visualization posts as well as more content on statistical programming and data analysis.

Cheers!

Guilherme

\newpage

# Nov 22, 2023 - Exploratory data analysis with few-line code? Yes, you can!

Exploratory Data Analysis (EDA) is a fundamental step before starting any statistical analysis. It involves summarizing the main characteristics of the data, often using visual methods like histograms, raincloud plots, box plots, and more, to understand the data structure, identify patterns, spot anomalies, and test hypotheses.

The main reasons why EDA is relevant:

* Getting to know the data: EDA helps in getting a sense of what the data looks like — its distribution, range, and features. This understanding is crucial before applying any advanced analytics or modelling;

* Identifying patterns and relationships: Through EDA, you can uncover relationships between variables, correlations, trends, and potential outliers. This insight can guide further analysis and hypothesis testing;

* Detecting anomalies or outliers: Exploring the data visually often reveals anomalies or outliers that might require special treatment or investigation;

* Feature Selection: EDA aids in selecting the most relevant features for modeling. It helps in understanding which features might be most predictive or influential;

* Data cleaning and pre-processing: EDA often highlights missing data, inconsistencies, or data quality issues that need to be addressed before further analysis;

* Hypothesis generation: EDA can inspire hypotheses that can be tested formally using statistical methods.

* Communicating insights: Visualizations generated during EDA are often used to communicate findings to stakeholders who might not be well-versed in the technical aspects of data analysis.

Below you will find 4 different R libraries/functions to perform EDA with few code lines.

## Data loading and preparation

In this article, we will use the dataset "woodman.pig" (Woodman et al., 1936; https://doi.org/10.1017/S002185960002308X) from R library agridat v.1.22 (Wright, 2023).

To make it simple, the following variables will be retained for EDA:

* pen
* treatment
* pig
* sex
* amount of feed consumed (in pounds)
* weekly weight gain (in pounds)

```{r data-load-Nov22, include=T}
install.packages(c("agridat","tidyr","tidyverse","dplyr"))

library("agridat")
library("tidyr")
library("tidyverse")
library("dplyr")

data <- agridat::woodman.pig

d <- data |> 
        mutate(pen=as.factor(pen),
               treatment=as.factor(treatment),
               pig=as.factor(pig),
               sex=as.factor(sex),
               feed_consumed=as.numeric(feed),
               weekly_weight_gain=as.numeric(g)) |>
    select(pen,treatment,pig,sex,feed_consumed,weekly_weight_gain)

head(d,5)
```

```{r rainplot, include=T, fig.cap="(ref:rainplot)"}
ggplot(d, aes(x = treatment, y = feed_consumed)) +
  geom_boxplot(width = 0.25) +
  xlab('Treatment') +
  ylab('Amount of feed consumed, in pounds') +
  #ggtitle("Weight gain from birth to weaning in 4 lamb genotypes") +
  theme_classic(base_size=18, base_family="serif")+
  theme(text = element_text(size=18),
        axis.text.x = element_text(angle=0, hjust=.5, vjust = 0.5, color = "black"),
        axis.text.y = element_text(color = "black"),
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position="none")+
  scale_y_continuous(breaks = seq(400, 600, by=25), limits=c(400,600), expand = c(0, 0)) +
 stat_halfeye(adjust = .5, width = .6, justification = -.3, .width = 0, point_colour = NA) +
  geom_point(shape = 95, size = 15, alpha = .2, color = "#1D785A")
```

## Exploratory Data Analysis methods

### Option 1: skimr

The *skimr* is a helpful R library for EDA.

It provides summary statistics and visualizations to understand the structure and characteristics of your dataset.

Here are some ways to use it:

install.packages("skimr")

library(skimr)
library(dplyr) # For data manipulation (optional but often used)

summary.skimr <- skim(d)

summary.skimr.numeric <- d |> 
                         skim(where(is.numeric)) # Summarize only numerical variables

summary.skimr.numeric.facet <- d |>
                               dplyr::group_by(treatment) |>
                               skim(where(is.numeric)) #Summarize numerical variables by treatment

  
### Option 2: DataExplorer

The *DataExplorer* library is another useful tool in R for EDA.

It provides various functions to quickly explore and visualize datasets, generate summary statistics, and handle missing values.

Find some ways to use this library below:

install.packages("DataExplorer")

library(DataExplorer)
library(dplyr) # For data manipulation (optional but often used)

summary.de <- plot_intro(d) #provides a quick summary of the data types, missing values, and some summary statistics

summary.report <- create_report(d, report_title = "Exploratory Data Analysis Report") #generates a comprehensive report with summary statistics, histograms, bar plots, correlation plots, etc., for numeric and categorical variables.

summary.report.faceted <- create_report(d, report_title = "Exploratory Data Analysis Report", y = "treatment")


### Option 3: SmartEDA

The *SmartEDA* library is another powerful tool for quick data overview with increasing number of exploratory functions. Analyzing information value, weight of evidence, custom tables, summary statistics, graphical techniques will be performed for both numeric and categorical predictors.

install.packages("SmartEDA")

library(SmartEDA)

data.overview.smarteda <-  ExpData(data=d,type=1) #overview of the data
   
data.strcuture.smarteda <- ExpData(data=d,type=2) #structure of the data

report.smarteda <- ExpReport(d, label=NULL, op_file="Report.html", op_dir=getwd()) #generates data overview report in HTML format

report.smarteda.faceted <- ExpReport(d, Target="treatment", label=NULL, op_file="Report.html", op_dir=getwd()) #generates data overview report by treatment in HTML format


### Option 4: tableone

The *tableone* library is another powerful tool primarily used for creating summary statistics tables.

It is particularly useful for generating publication-ready tables that summarize characteristics of different groups within a dataset.

Here is an overview of using this library for creating summary tables:

install.packages("tableone")

library(tableone)

summary.tableone <- tableone::CreateTableOne(vars = colnames(select(d, -"treatment")), 
                                          strata = c("treatment"), data = d)
# print(summary.tableone)


EDA is typically the phase where analysts or data scientists spend a considerable amount of time because it sets the stage for the entire analysis process.

It helps in making informed decisions about which analysis techniques or models might be appropriate for the dataset and the problem at hand.

You just learned four different ways of running an exploratory data analysis with few lines of code! Well done!

As R and statistical programming evolve, new libraries and functions are being developed.

So, this list is not permanent and may not even match the specific requirements of a project. 
Therefore, you should see this article as a start point for your EDA, a way of having a quick overview of your data, and inspiration for optimization of your data preparation by combination of these methods with other EDA techniques and visualization libraries for a comprehensive analysis.

If you are aware of extra R libraries and functions to perform EDA or improve the codes used above, leave a comment below! Any improvements and extra knowledge are welcome!

Stay tuned for more data visualization posts as well as more content on statistical programming and data analysis.

Cheers!

Guilherme

```{r remove-data-Nov22, include=T}
rm(data, d)
```

\newpage

# Nov 30, 2023 - "Visualizing the number of animals slaughtered for meat in the top 4 world's largest meat producers using the R library crosstalk"

The Rmd. file "Animals slaughtered" containing the algorithm is placed in the repository "MediumBlog".

\newpage

# Dec 12, 2023 - "Spotting data outliers in R"

## What is a data outlier and its impact on statistical analysis and data interpretation?
An outlier in a dataset refers to a data point or observation that significantly deviates from other observations in the sample, which can sometimes skew statistical analyses and data interpretation.

Outliers can occur due to various reasons such as measurement and data entry errors, experimental variability, or they might even represent a genuine anomaly or rare event within the data.

An outlier often signifies problematic data, such as potential errors in coding or errors in experimental execution.

Should it be established that an outlier indeed stems from an error, it is advisable to remove or rectify the aberrant value in the analysis.

However, determining the validity of an outlier can sometimes be challenging.

Outliers might emerge from random fluctuations or reveal scientifically noteworthy insights.

Consequently, outright deletion of outlier observations is not advised.

Nonetheless, when substantial outliers are present, resorting to more robust statistical methods is suggested.

Detecting outliers is important in data analysis as they can impact the accuracy of statistical models or conclusions drawn from the data.

Data outlier detection can be performed in several ways, including graphical analysis, descriptive statistics, and formal test techniques.

In today's post I will cover three ways to spot several outliers using R. Therefore, Dixon's test and Grubb's test, which examine whether a *single* low or high value is an outlier at a time, will not be covered.


## Example dataset and visualization of potential outliers

In this article, we will use the dataset "saunders.maize.uniformity", regarding uniformity trial of maize in South Africa, stored in the R library agridat v.1.22 (Wright, 2023).

We will use the variable "yield" (yield per plot, in pounds) on the year of 1930.

```{r data-load-Dec12, include=T}
install.packages(c("agridat","tidyr","tidyverse","dplyr","ggplot2"))

library("agridat")
library("tidyr")
library("tidyverse")
library("dplyr")
library("ggplot2")

data <- agridat::saunders.maize.uniformity

data <- data |> 
        mutate(year=as.factor(year),
               yield=as.numeric(yield)) |>
        filter(year=="1930")

head(data,10)
```


```{r histogram, include=T}
hist(data$yield)
```


## Boxplot inspection and interquartile range
Boxplots are useful graphical tools to detect potential outliers. Figure \@ref(fig:boxplot) illustrates a boxplot of yield per plot on each experimental year. 

(ref:boxplot) Boxplot of yield per plot in 1930. The boxplot displays five summary statistics measures: minimum, median, first and third quartiles and maximum.

```{r boxplot, include=T, fig.cap="(ref:boxplot)"}
data |> ggplot(aes(x = "", y = yield)) +
  geom_boxplot(fill = "#0c4c8a") +
  xlab(' ') +
  ylab('Yield per plot, in pounds') +
  theme_minimal(base_size=18, base_family="serif")
```


```{r individual-outliers, include=T}
out <- boxplot.stats(data$yield)$out
out_ind <- which(data$yield %in% c(out))
data[out_ind, ]
```

It is also possible to extract the values of the potential outliers thanks to the *boxplot.stats()$out* function.

Any observation considered as outlier is found outside the interval between quartiles, aka interquartile range (IQR).

The IQR is a statistical measure used to describe the spread or dispersion of a dataset and is calculated as the difference between the third quartile (Q3) and the first quartile (Q1) in a dataset.

In this example, *16* observations (`r out`) were considered outliers according to the interquartile range criterion.

With the function *which()* we can extract the row numbers corresponding to these outliers: `r out_ind`.


## Z-score
Z-scores serve as a measure of how atypical an observation is within a dataset that adheres to a normal distribution.

They represent the number of standard deviations a particular value deviates from the mean.

For instance, a Z-score of 2 signifies that an observation lies two standard deviations above the average, whereas a Z-score of -2 indicates it is positioned two standard deviations below the mean.

A Z-score of zero denotes a value that aligns precisely with the mean.

The further away an observation’s Z-score is from zero, the more unusual it is.

A standard cut-off value for finding outliers are Z-scores of +/-3 or further from zero.

```{r z-score, include=T}
data$z_yield <- scale(data$yield)
hist(data$z_yield)

z.score.ind <- which(data$z_yield < -3 | data$z_yield > 3)
data[z.score.ind,]
```

Using the Z-score methods, 4 observations were considered outliers and their row numbers were: `r z.score.ind`.


## Rosner's generalized extreme Studentized deviate (ESD) Test
The Rosner's generalized ESD test is an extension of the Grubbs' test for detecting several outliers in a univariate data set.

The steps involved in Rosner's ESD test:

1. Identify potential outliers: The test aims to identify potential outliers by iteratively removing the suspected maximum value (or minimum value, depending on the direction of the test) from the dataset. It calculates the extreme studentized deviate for each potential outlier.

2. Calculate the Extreme Studentized Deviate: The extreme studentized deviate is essentially a measure of how extreme a data point is compared to the rest of the data. It is calculated as the difference between the suspected outlier and the sample mean, divided by the sample standard deviation. This is then divided by an estimate of the standard deviation of the suspected outlier. This gives a measure of how many standard deviations away from the mean the suspected outlier is.

3. Compare with critical values: The extreme studentized deviate values obtained are compared against critical values from the ESD distribution table based on the desired significance level and the number of data points in the dataset. If the calculated extreme studentized deviate exceeds the critical value, the suspected outlier is considered significant and removed from the dataset.

4. Iterative process: The process is repeated until either no more outliers are detected or until the number of outliers detected reaches a predetermined maximum number.

In addition to normal distribution assumption, the Rosner's ESD test assumes that the variance within the dataset is constant.

Below, you can see the R script to perform the Rosner's ESD test.

```{r rosner-test, include=TRUE}
#install.packages(c("EnvStats"))

test <- EnvStats::rosnerTest(data$yield, k = 16, alpha = 0.05) # k = 16, which is the number of suspected outliers from interquartile range criterion analysis.

test$all.stats
```

Based on the Rosner's test, only one observation was deemed outlier, which was the observation row 456 with a value of 19.9.


## Discussion of results
Z-scores and Rosner's ESD test are both used for outlier detection but operate in distinct ways.

Z-score is a measure that describes the position of a data point within a normal distribution in terms of standard deviations from the mean. It is calculated for individual data points and helps in understanding how far a particular value deviates from the mean. Z-scores are not explicitly designed for outlier detection but can be used to identify extreme values. They assess each data point's deviation from the mean based on the standard deviation of the entire dataset.

Rosner's ESD Test is explicitly designed for outlier detection. It aims to detect potential outliers by iteratively removing suspected extreme values from the dataset. It calculates extreme studentized deviates, measuring how many standard deviations an individual value deviates from the mean, but the calculation is based on the sample mean and standard deviation. The test involves sequentially identifying and removing suspected outliers until no more are found, or until a predetermined maximum number is reached.

The main difference lies in their methodologies:

- Z-scores assess individual data points' deviations from the mean using the standard deviation of the entire dataset.
- Rosner's ESD test iteratively evaluates potential outliers by comparing their deviation from the sample mean and standard deviation, removing suspected outliers sequentially.

These methods can provide different results because they operate based on different principles and approaches. Additionally, Rosner's ESD test involves an iterative process that considers each potential outlier's influence on subsequent calculations, while Z-scores analyze individual data points independently.

The outcomes might vary due to the specific nature of the methods and their sensitivity to different types of outliers or extreme values. Also, the application and interpretation of results from these methods depend on the context and assumptions of the data being analyzed.

## Conclusions
Numerous methods exist to spot outliers, but leveraging deep knowledge of variables proves vital.

Visual tools like boxplots and histograms, along with data sorting, often reveal potential outliers swiftly.

Most statistical tests for outlier detection rely on normal distribution assumptions and can be sensitive to outliers, therefore, should not be used alone.

Instead, prior experience with the data's nature and collection process as well as a trained ability to spot unusual values and use formal statistical tests should walk together.

Once again, not every outlier warrants deletion; some hold valuable insights. Outliers often offer crucial information about the subject and data collection methods. Understanding their occurrence and potential recurrence as a natural part of the process is essential.

See you in the next post!

\newpage

# Jan 05, 2024 - "An easier-to-interpret alternative to paired barplots in R"

## Introduction
The easiness to plot makes paired barcharts very popular among researchers and data scientists. However, they may become cluttered if many groups are included in the plot and often impose a challenge to scale.

In today's post, we will explore an alternative to paired barplots that can facilitate data visualization and comparison between groups.

We will look at the variation in average farm size between 1960 and 2000 in various countries. The dataset can be retrieved at Our World in Data (https://ourworldindata.org/agricultural-production).

## Loading necessary libraries and preparing our plotting theme

In the next three code chunks, we will load the necessary libraries for this exercise, load and process our data, and prepare our plotting theme.

```{r libraries, include=T}
#install.packages(c("xxx"))

library(readr)
library(tidyverse)
library(dplyr)
library(tidyr)
library(ggtext)
library(ggplot2)
library(glue)
library(patchwork)

```

```{r data-load, include=T}
setwd("C:/Users/au588902/Dropbox/Medium/MediumBlog/datasets")

d <- read_csv("average-farm-size.csv") |>
     mutate(country=as.factor(Entity),
               year=as.factor(Year),
            size=as.numeric(average_farm_size_ha)) |>
        filter(year=="1960" | year== "2000") |>
        filter(country=="Argentina" | country=="Brazil" | country=="Chile" | country=="India" | country=="Netherlands" | country=="Denmark" | country=="France" | country=="Germany" | country=="Spain" | country=="Italy" | country=="United States" | country=="Canada" | country=="Ireland" | country=="Uruguay" | country=="United Kingdom" | country=="Austria")

```

```{r plot-base, include=T}
my_theme <- theme_minimal(base_size = 12, base_family = 'serif') +
  theme(
    legend.position = 'none',
    plot.title.position = 'plot',
    text = element_text(color = 'grey20'),
    plot.title = element_markdown(size = 20, margin = margin(b = 5, unit = 'mm'))
  )
theme_set(my_theme)

color_palette <- c("#0072B2", "#D55E00")
names(color_palette) <- c(1960, 2000)

title_text <- glue(
  "Comparison of average farm size between <span style = 'color:{color_palette['1960']}'>1960</span> and <span style = 'color:{color_palette['2000']}'>2000</span>")

```


(ref:bar-plot) Paired barplot illustrating variation in average farm size across several countries.

```{r bar-plot, include=T, fig.cap="(ref:bar-plot)"}
badplot <- d |>
          mutate(country = fct_reorder(country, size, max)) |>
          ggplot(aes(x=size, y=country, col=year,fill=year)) +
          geom_bar(position="dodge", stat="identity") +
          labs(x = 'Average farm size, in hectares',
          y = element_blank(),
          title = title_text) +
          scale_color_manual(values = color_palette) +
          scale_fill_manual(values =color_palette)
badplot

```

Although relatively easy to plot, the paired barplot (Fig. \@ref(fig:bar-plot)) looks quite cluttered and forces the reader to move the eyes around to make comparisons. Certainly this can be improved!

The alternative is the dumbbell plot, a combination of dotplots with connecting horizontal lines. Below you can find the code chunk to draw a dumbbell plot of the same data. Keep in mind that:
- Points need to be plotted above the horizontal lines.
- Y-axis gridlines were removed, as they became superfluous.
- Dumbbells are sorted by decreasing average farm size in 2000 after using *fct_reorder()* when we computed *segment_helper*. The same step also ensured that countries where the green dot is left of the orange dot are grouped together (and vice-versa).

Not least important, the dumbbell plotting code is presented in form of function so you can decide how to change the graph without duplicating the script and save time!

```{r dumbbell-function, include=T}
create_dot_plot <- function(d, sort_var = NULL) {
  segment_helper <- d |>
    select(country, year, size) |>
    pivot_wider(names_from = year, values_from = size, names_prefix = 'year_') |>
    mutate(change = year_2000 - year_1960,
      country = fct_reorder(country, year_2000 * if_else(change < 0, -1, 1)))
  
  if (!missing(sort_var)) {
    segment_helper <- segment_helper |>
      mutate(country = fct_reorder(country, {{sort_var}} * if_else(change < 0, -1, 1)))
  }
  
  ggplot() +
    geom_segment(data = segment_helper,
      aes(y = country, yend = country, x = year_1960, xend = year_2000),
      col = 'grey30',
      size = 1.25 ) +
    geom_point(data = d,
      aes(x = size, y = country, col = year), size = 4) +
    labs(x = 'Average farm size, in ha',
      y = element_blank(),
      title = title_text,
      caption = 'Our World in Data') +
    scale_color_manual(values = color_palette) +
    theme(panel.grid.major.y = element_blank(),
      panel.grid.minor.x = element_blank()) +
    scale_x_continuous(expand = expansion(mult = 0.01))
}

```

(ref:dumbbell-plot) Dumbbell plot illustrating variation in average farm size across several countries.

```{r dumbbell-plot, include=T, fig.cap="(ref:dumbbell-plot)"}
create_dot_plot(d, desc(year_2000))

```

The dumbbell plot ((Fig. \@ref(fig:dumbbell-plot))) displays the data sorted according to the lowest average farm size in 2000. We can see that *Chile*, *Brazil*, and *India* showed a reduction in average farm size over this selected period, while other countries witnessed an increase in average farm size.

This dumbbell plot can be further improved by replacing the dumbbells with arrows and respective legends. The chunk below demonstrates how to make this change. This chunk includes the steps to make the plot and a custom legend explaining the temporal order of each arrow. The arrow plot and the custom legend were put together using the R library *patchwork*.


(ref:arrow-plot) Arrow plot illustrating variation in average farm size across several countries.

```{r arrow-plot, include=T, fig.cap="(ref:arrow-plot)"}
arrow_plot <- d |> 
  
  select(country, year, size) |> 
  
  pivot_wider(names_from = year, names_prefix = 'year_', values_from = size) |> 
  
  mutate(change = year_2000 - year_1960, 
    sign_change = (change > 0),
    country = fct_reorder(country, year_2000 * if_else(sign_change, -1, 1))) |> 
  
  ggplot( aes(x = year_1960, xend = year_2000, 
      y = country, yend = country,
      color = sign_change)) +
  geom_segment( arrow = arrow(angle = 30, length = unit(0.2, 'cm')), size = 1) +
  labs(x = 'Average farm size, in ha', 
    y = element_blank(),
    title = 'Comparison of average farm size between 1960 and 2000',
     caption = 'Our World in Data') +
  scale_color_manual( values = unname(color_palette))

dat <- tibble(country = c(1.1, 1),
  year_1960 = c(2, 1),
  year_2000 = c(1, 2)) 

dat_longer <- dat |> 
  pivot_longer( cols = -country,
    names_to = 'label',
    values_to = 'size',
    names_prefix = 'year_') 

custom_legend <- ggplot() +
  geom_rect(aes(xmin = 0.8, xmax = 2.2,
        ymin = 0.9, ymax = 1.2),
    fill = 'white',
    col = 'grey30') +
  geom_segment( data = dat,
    mapping = aes(x = year_1960, xend = year_2000, 
      y = country, yend = country),
    arrow = arrow(angle = 30, length = unit(0.2, 'cm')),
    color = color_palette,
    size = 1) +
  geom_text(data = dat_longer,
    mapping = aes(x = size, y = country, label = label),
    hjust = c(-0.1, 1.1, 1.1, -0.1),
    family = 'serif',
    color = rep(color_palette, each = 2))  +
  theme_void() +
  coord_cartesian(ylim = c(0.8, 1.3),
    xlim = c(0.75, 2.25), 
    expand = F)

arrow_plot +
  inset_element(custom_legend, left = 0.55, right = 1, top = 1, bottom = 0.8)

```

The arrow plot ((Fig. \@ref(fig:arrow-plot))) more clearly showing the directions in variation of average farm size across selected countries.

Great! We learned how to make Dumbbell and Arrow plots and, perhaps, ditch barplots. We also managed to cover the use of two extra R libraries: *glue* to annotate a nice plot title with year colors and dispensing the use of plot legends, and *patchwork* to insert annotations, shapes or illustrations into our plots.

See you next time!

\newpage

# Jan 11, 2024 - "Comparing economic relevance of tourism among world regions using bump charts in R"

Tourism plays a significant role in the global economy, contributing to job creation, economic growth, and international cooperation.

Let's have a look at the share of tourism in total Gross Domestic Product (GDP) among several world regions between 2016 and 2020 using data retrieved from *Our World in Data* (Herre et al., 2023; https://ourworldindata.org/tourism).

```{r libraries-jan09, include=T}
#install.packages(c("xxx"))

library(readr)
library(tidyverse)
library(dplyr)
library(tidyr)
library(glue)
library(ggplot2)
library(ggtext)
library(ggrepel)
library(ggbump)
library(patchwork)

```


```{r data-processing, include=T}
setwd("C:/Users/au588902/Dropbox/Medium/MediumBlog/datasets")

d <- readr::read_csv('tourism-gdp-proportion-of-total-gdp.csv') |>
  filter(!Year<2016 & !Year==2021) |>
  filter(!Entity=="Northern America and Europe") |>
  mutate(share = as.numeric(`Tourism GDP as a proportion of Total`))

d <- d[!complete.cases(d),] #selecting world regions, excluding single countries

pal <- c(
  "#5D3A9B","#D41159", "#E69F00", "#009E73", "#0072B2", "#CC79A7", "#999999", "#D55E00","black")

current <- d %>%
  group_by(Entity) %>% 
  slice(which.max(Year)) %>% 
  ungroup()

title_text <- glue(
  "Share of Tourism in total GDP between 2016 and 2020, in %")

```

(ref:linechart) Line chart illustrating the share (in %) of tourism in total GDP across world regions between 2016 and 2020.

```{r linechart, include=T, fig.cap="(ref:linechart)"}
ggplot(d, aes(x = Year,
           y = share, group = Entity)) + 
  geom_point(aes(color = Entity)) + 
  geom_line(aes(color = Entity), size = 2, alpha = 0.8) +
  geom_text_repel(data = current, aes(label = Entity, color = Entity), size = 3, hjust = -2, vjust = 1) +
  scale_color_manual(values = pal) +
  xlim(2016, 2020.5) + 
  labs(x = element_blank(),
       y = element_blank(),
       title = title_text,
       caption = 'OurWorldInData.org/tourism') +
  theme_minimal(base_size = 16, base_family = 'serif') +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = "none")

```

The line chart (Fig. \@ref(fig:linechart)) shows us that the economic relevance of tourism was the highest in Oceania ex-Australia/New Zealand throughout the sampled period.

Meanwhile, we can see that, except for "Central and Southern Asia" region, all remaining regions exchanged positions in the rank of economic importance of tourism at specific time points.

Although the line chart displays the actual shares of each region in each year, if one is interested in knowing the ranking position of these regions, the line chart is not so helpful as it can become cluttered and difficult to read.

Therefore, a great alternative to visualize different positions over time is to plot a *bump chart*.

A bump chart, also known as a slope chart or a rank chart, is a type of data visualization tool used to track the changes in ranking or position of different entities over time.

The chart consists of lines that connect the ranks of individual items (like world regions in this case) across different time periods. Each line represents an entity, and the movement of the line illustrates how its ranking changes over time. The steeper the slope of the line, the more significant the change in rank.

Bump charts are commonly used in sports analytics, business analytics, and other fields where tracking changes in ranking or position is essential for analysis.

Check out below how to transform this line chart into a bump chart in R and better see the ranking of world regions regarding their share of tourism in total GDP.

(ref:bumpchart) Bump chart illustrating the ranking position of each world region in terms of share of tourism in total GDP between 2016 and 2020.

```{r bumpchart, include=T, fig.cap="(ref:bumpchart)"}
rank_by_year <- d |> 
  mutate(rank = row_number(desc(share)), .by = Year)

max_rank <- 9

todays_top <- rank_by_year %>% 
  filter(Year == 2020, rank <= max_rank) %>% 
  pull(Entity)

names(pal) <- todays_top
description_color <- 'grey30'

bump_chart <- rank_by_year |> 
  ggplot(aes(Year, rank, col = Entity)) + 
  
  geom_point(shape = '|', stroke = 9) +
  
  geom_bump(linewidth = 2) +
  
  geom_text(data = rank_by_year |> filter(Year == 2016),
    aes(label = Entity),
    hjust = 1,
    nudge_x = -0.1,
    size = 3.9,
    fontface = 'bold',
    family = 'serif') +
  
  geom_text(data = rank_by_year |> filter(Year == 2020),
    aes(label = rank),
    hjust = 0,
    nudge_x = 0.1,
    size = 3.9,
    fontface = 'bold',
    family = 'serif') +
  
  annotate('text',
    x = seq(2016, 2020, 1),
    y = 0.5,
    label = seq(2016, 2020, 1),
    hjust = 0.5,
    vjust = 1,
    size = 5,
    fontface = 'bold',
    color = description_color,
    family = 'serif') +
  
  scale_color_manual(values = pal) +
  
  coord_cartesian(xlim = c(2012.5, 2020.9), ylim = c(9.5, 0.25), expand = F) +
  
  theme_void(base_size = 13, base_family = 'serif') +
  
  theme(legend.position = 'none',
    plot.background = element_rect(fill = 'grey95', color = NA),
    plot.margin = margin(t=1, r=1, b=1, l=3, "mm"),
    text = element_text(color = description_color),
    plot.title = element_text(face = 'bold', family = 'serif', size = rel(1.3))) +
  
  labs(title = 'Share of Tourism in total Gross Domestic Product (GDP)',
    subtitle = 'Rank by GDP from tourism includes tourism industries and other industries that directly serve visitors.',
    caption = 'OurWorldInData.org/tourism')

bump_chart
```

Here we go, the bump chart (Fig. \@ref(fig:bumpchart)) shows us more clearly the rank of world regions regarding how economically relevant tourism was between 2016 and 2020.

Thanks for reading it and see you next time!

Gui

\newpage

# Jan 16, 2024 - "Quickly compare data sets in R"

Dealing with vast data sets from different sources is an integral part of a statistical programmer or data scientist's work routine.

In this regard, the importance of comparing data sets cannot be overstated.

Data sets serve as foundation structures in popular programming languages providing a systematic and organized way to store and manipulate data.

Comparing data sets plays a pivotal role in ensuring data quality, identifying patterns, and making informed decisions based on robust analyses.

By comparing data sets, data scientists can quickly pinpoint discrepancies between data sets, allowing for thorough data cleaning and pre-processing.

This process is crucial for maintaining data integrity, enhancing the reliability of subsequent analyses, and ensuring that insights drawn from the data accurately reflect the underlying reality.

Furthermore, comparing data sets is relevant merging and integrating data from multiple sources.

By comparing data sets based on key identifiers, data scientists can effectively merge data sets, ensuring that information is integrated accurately.

This process is fundamental in creating comprehensive data sets that capture a holistic view of the phenomena under investigation.

In today's post we will explore the R library *arsenal* (v.3.6.3; Heizen et al., 2021) for easy, quick comparison of two data sets resultant of merging two data sources in different ways.

```{r libraries-jan16, include=T}
#install.packages(c("xxx"))

library(readxl)
library(dplyr)
library(arsenal)
```

In this post, we initially use two data sets of a randomized clinical trial.

The *ref* data set refers to information of patients (variable: SUBJID) prior to random assignment to one of two treatments. Each patient has one single row of data.

The second data set, *lab*, refers to laboratory analysis information of each patient over several sampling points.

```{r data-loading-jan16, include=T}
setwd("C:/Users/au588902/Dropbox/Medium/MediumBlog/datasets")

ref <- readxl::read_excel("clinical_data.xlsx", sheet = "ADSL") #N = 257 observations

lab <- readxl::read_excel("clinical_data.xlsx", sheet = "ADLB") #N = 2439 observations
```

For the purpose of demonstrating the use of the arsenal R library, we will left-join (R library *dplyr*) the two data sets in two different ways, the right way using the *ref* data set as our reference ('x') and the wrong way using the *lab* data set as our reference ('x').

OBS.: another way of creating two different merged data sets would be using "left_join" and "right_join" without changing the order of the data sets in the function.

```{r right-data-jan16, include=T}

right.dataset <- dplyr::left_join(ref, lab, by = "SUBJID")  #N = 2422 observations
head(right.dataset,5)

```

The *right.dataset* above contains 2422 observations.

```{r wrong-data-jan16, include=T}

wrong.dataset <- dplyr::left_join(lab, ref, by = "SUBJID")  #N = 2439 observations
head(wrong.dataset,5)
```

The *wrong.dataset* above contains 2439 observations.

It is pretty clear that something went wrong when we changed the order of data sets in the formula, as the merged data sets did not share 17 observations (rows).

To check this difference in detail, let's use the function *comparedf* from the R library *arsenal*.

The output below shows us that 17 observations (rows) are different between the two merged data sets across all variables. In fact, two patients (S001054 and S051018) were present in the sampling period, but had no prior information before the start of the trial.


```{r comparing-datasets-jan16, include=T}

comparison <- arsenal::comparedf(right.dataset, wrong.dataset, by = NULL)
```

```{r print-comparison-jan16, include=T}

print(comparison)
```

```{r summary-comparison-jan16, include=T}

summary(comparison)
```


When looking at the differences between merged data sets by a given variable, in this case 'patient', we observe that the merged data sets did not share 23 observations - see output below. In addition to the two patients with no description information prior the sampling period, there were three patients with information who were not present during the sampling period.


```{r comparing1-datasets-jan16, include=T}

comparison1 <- arsenal::comparedf(right.dataset, wrong.dataset, by = "SUBJID")
```

```{r print1-comparison-jan16, include=T}

print(comparison1)
```

```{r summary1-comparison-jan16, include=T}

summary(comparison1)
```


## Conclusion

From ensuring data quality and integrity to uncovering hidden patterns and facilitating data integration, the comparative analysis of data sets is a fundamental step in the data science workflow.

As the field continues to evolve, the ability to effectively compare and contrast data sets will remain a critical skill, empowering data scientists to extract meaningful insights and drive informed decision-making.

The R library *arsenal* provides a practical insight on discrepancies between data sets and it is a useful tool for daily data science work. Leave me a comment if you know other packages/functions to carry out similar task in R!

Thanks for reading it and see you next time!

Gui

Reference: Heinzen E, Sinnwell J, Atkinson E, Gunderson T, Dougherty G (2021). _arsenal: An Arsenal of 'R' Functions for Large-Scale Statistical Summaries_. R package version 3.6.3, https://CRAN.R-project.org/package=arsenal>.


\newpage